{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4ea0f7-2d0f-4ad2-b6fb-8ae6a3f45dc8",
   "metadata": {},
   "source": [
    "### **Vision-Language Medical Foundation Models**\n",
    "\n",
    "#### **1.5. Few-shot Parameter Efficient Fine-Tuning**\n",
    "---\n",
    "\n",
    "Parameter-Efficient Fine-Tuning is a methodology of increasing interest, currently popularized in the NLP community to adapt the recently introduced large-scale LLMs.\n",
    "\n",
    "**Objective**: Given an small set of examples per category, we want to fine-tune parts of the model to specialize it on a particular task. By tuning only few parameters, we can adapt large-complexity models with minimal resources.\n",
    "\n",
    "**Few-shot**: We only use K number of images for each new category.\n",
    "\n",
    "**Why PEFT?**: They are efficient (ar least more than full fine-tuning), usually run with in-house GPUs. They are fast: you can transfer the model in a matter of minutes. They are more flexible than black-box Adapters, since it allows to refine deep features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821eb2b-d9b8-4a2c-ac35-9f7a1c558757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device for training/inference.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Available device: \" + device)\n",
    "\n",
    "# Seeds for reproducibility\n",
    "def set_seeds(seed_value, use_cuda):\n",
    "    np.random.seed(seed_value)     # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu vars\n",
    "    random.seed(seed_value)        # Python\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)     # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(42, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e0b86-3371-42de-8fd7-c4b0bad0848d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:45:03.438939Z",
     "iopub.status.busy": "2024-07-10T19:45:03.438466Z",
     "iopub.status.idle": "2024-07-10T19:45:03.442764Z",
     "shell.execute_reply": "2024-07-10T19:45:03.441757Z",
     "shell.execute_reply.started": "2024-07-10T19:45:03.438910Z"
    },
    "tags": []
   },
   "source": [
    "#### **Dataset details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849de46-e1c7-4ec4-8386-167241ff68b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SICAPv2 dataset metadata\n",
    "categories = [\"NC\", \"G3\", \"G4\", \"G5\"]                                        # List of categories\n",
    "path_images = \"./local_data/datasets/SICAPv2/images/\"                        # Folder with the images\n",
    "dataframe_train = \"./local_data/datasets/SICAPv2/partition/Test/Train.xlsx\"  # Dataframe (Table) containing train images names and labels\n",
    "dataframe_test = \"./local_data/datasets/SICAPv2/partition/Test/Test.xlsx\"    # Dataframe (Table) containing test images names and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38c7be-9b27-4a71-904e-d6463bd4ecd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:45:25.448588Z",
     "iopub.status.busy": "2024-07-10T19:45:25.448199Z",
     "iopub.status.idle": "2024-07-10T19:45:25.452081Z",
     "shell.execute_reply": "2024-07-10T19:45:25.451297Z",
     "shell.execute_reply.started": "2024-07-10T19:45:25.448560Z"
    },
    "tags": []
   },
   "source": [
    "#### **VLM model wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886f24e-579d-4806-80a5-7938f2b3bd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and pre-processing tools from huggingface.\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# In Transformers library, models and versions are storages by an ID defininf the use and model name.\n",
    "# For PLIP model, such ID is \"vinid/plip\".\n",
    "processor = AutoProcessor.from_pretrained(\"vinid/plip\") # pre-processing image and text.\n",
    "processor.image_processor.do_center_crop = False\n",
    "plip = AutoModel.from_pretrained(\"vinid/plip\").eval() # model with pre-trained weights.\n",
    "# We set model in eval mode to avoid droput inference and batchnorm stats update in CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcae7d-8a0a-4c2a-ae34-1edb4f1aeb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, we will use our PLIP Wrapper for easy interaction\n",
    "class PLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, proj_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder         # Take one-modality encoder from VLM.\n",
    "        self.proj_layer = proj_layer   # Take projection layer into joint embedding space.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Forward input trough encoder\n",
    "        features = self.encoder(**inputs).pooler_output # Forward trough encoder - we keep a global feature for the image/text\n",
    "        \n",
    "        # Project features\n",
    "        projected_features = self.proj_layer(features)  # Apply projection\n",
    "        \n",
    "        # Ensure image features are l2-norm\n",
    "        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True) # l2-normalization\n",
    "\n",
    "        return projected_features\n",
    "\n",
    "# Create model wrapper for vision and text encoders\n",
    "vision_encoder = PLIPWrapper(plip.vision_model, plip.visual_projection)\n",
    "text_encoder = PLIPWrapper(plip.text_model, plip.text_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35459e-b734-4115-8d1f-a893b9b2ccc7",
   "metadata": {},
   "source": [
    "#### **Compute text prototypes**\n",
    "(We will need them latter for classification head initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f92c0-e587-46b8-a767-20eff1f79d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble of templates.\n",
    "templates = [\"a histopathology slide showing [CLS]\", \"histopathology image of [CLS]\",\n",
    "             \"pathology tissue showing [CLS]\", \"presence of [CLS] tissue on image\"]\n",
    "\n",
    "# Category-wise descriptions, which are more informative than category names. For instance, \"atrophic dense glands\" better \n",
    "# describes the local findings associated with Gleason grade 3.\n",
    "prompts_dict = {\"NC\": [\"benign glands\"],\n",
    "                \"G3\": [\"atrophic dense glands\"],\n",
    "                \"G4\": [\"cribriform ill-formed fused papillary patterns\"],\n",
    "                \"G5\": [\"isolated nest cells without lumen roseting patterns\"]}\n",
    "\n",
    "# Combine all paired options of templates and description.\n",
    "prompts = {}\n",
    "for iCategory in categories:\n",
    "    prompts[iCategory] = [caption.replace(\"[CLS]\", iDescription) for iDescription in prompts_dict[iCategory]\n",
    "                          for caption in templates]\n",
    "\n",
    "# Compute embeddings per category.\n",
    "class_prototypes = []\n",
    "for iKey in range(len(categories)):\n",
    "    with torch.no_grad():\n",
    "        # Retrieve descriptions for that particular category.\n",
    "        descriptions = prompts[categories[iKey]]\n",
    "        # Tokenize text.\n",
    "        inputs = processor.tokenizer(descriptions, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "        # Forward text encoder.\n",
    "        text_features_ensemble = text_encoder(inputs)\n",
    "        # Get class prototypes as average of all text prompts.\n",
    "        avg_text_features = text_features_ensemble.mean(0).unsqueeze(0)\n",
    "        # Re-normalize embedding.\n",
    "        avg_text_features = avg_text_features / avg_text_features.norm(dim=-1, keepdim=True)\n",
    "        class_prototypes.append(avg_text_features)\n",
    "                               \n",
    "# Concatenate all class prototypes.\n",
    "zero_shot_prot = torch.concat(class_prototypes, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aab924-24fc-4862-9bd4-d867674176b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:46:51.367156Z",
     "iopub.status.busy": "2024-07-10T19:46:51.366296Z",
     "iopub.status.idle": "2024-07-10T19:46:51.370298Z",
     "shell.execute_reply": "2024-07-10T19:46:51.369567Z",
     "shell.execute_reply.started": "2024-07-10T19:46:51.367122Z"
    },
    "tags": []
   },
   "source": [
    "#### **Datasets: few-shot training and test**\n",
    "Now we are going to modify the vision backbone. Thus, **we cannot pre-compute the deep features, since these will change during adaptation**. Thus, we define the data loader, but we do not pre-compute the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523b3d7-e092-465d-84e2-b305ba66bb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vlms.data import loader\n",
    "from vlms.data import few_shot_loader  \n",
    "from torchvision import transforms\n",
    "\n",
    "# Pre-processing transforms to apply during data loading.\n",
    "plip_transforms = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),                                                 # Move PIL/array image to tensor\n",
    "    transforms.Normalize(std=processor.image_processor.image_std,\n",
    "                         mean=processor.image_processor.image_mean),       # Intensity normalization\n",
    "    transforms.Resize(list(processor.image_processor.crop_size.values()))  # Resize to pre-trained resolution\n",
    "    ])\n",
    "\n",
    "# Set test data loader.\n",
    "test_loader = loader(dataframe_path=dataframe_test, path_images=path_images, categories=categories,\n",
    "                     transforms=plip_transforms, batch_size=8, num_workers=0)\n",
    "\n",
    "# Set train data loader.\n",
    "shots, seed = 16, 1\n",
    "train_loader = few_shot_loader(dataframe_path=dataframe_train, path_images=path_images, categories=categories, transforms=plip_transforms,\n",
    "                               shots=shots, batch_size=32, num_workers=0, seed=seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c361612-43b5-4689-bbbd-e80fb934516b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T19:53:05.018907Z",
     "iopub.status.busy": "2024-07-10T19:53:05.018524Z",
     "iopub.status.idle": "2024-07-10T19:53:05.022188Z",
     "shell.execute_reply": "2024-07-10T19:53:05.021450Z",
     "shell.execute_reply.started": "2024-07-10T19:53:05.018878Z"
    },
    "tags": []
   },
   "source": [
    "#### **Preliminaries: coupling base model with classification head**\n",
    "First, we have to equip the vision backbone with a classification head. The best option is to re-use the Linear Probe head explored in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd29613-3c73-443c-8db5-b549898f17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the classification head.\n",
    "class ZSLinearProbe(torch.nn.Module):\n",
    "    def __init__(self, zero_shot_prot, logit_scale):\n",
    "        super().__init__()\n",
    "        self.logit_scale = logit_scale\n",
    "        self.logit_scale.requires_grad = False\n",
    "        self.prototypes = torch.nn.Parameter(zero_shot_prot.clone())\n",
    "        # move to device.\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        # Get trained prototype\n",
    "        prototypes = self.prototypes.to(device)\n",
    "\n",
    "        # l2-normalized trained weights.\n",
    "        prototypes_norm = prototypes / prototypes.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # temparature-scaled similarity per class.\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = features @ prototypes_norm.t() * logit_scale\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66549c70-d4b0-4589-bb2d-6390c2af4062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the classification head, initializaed with zero-shot, text prototypes.\n",
    "head = ZSLinearProbe(zero_shot_prot=zero_shot_prot, logit_scale=plip.logit_scale.detach().clone())\n",
    "\n",
    "# Create model combining backbone and classification head - Also, we move the model to gpu.\n",
    "model = torch.nn.Sequential(copy.deepcopy(vision_encoder),\n",
    "                            head).to(device).to(torch.float32)\n",
    "print(model) # Look again at the architecture of the model: it is a ViT/B/32 composed of 12 Transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c3714-e355-4590-9aee-798b81fd39f6",
   "metadata": {},
   "source": [
    "#### **Preliminaries: counting and freezing parameters**\n",
    "We present functions to control which parameters are trainable in the network, and which are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4d183-238e-4486-b351-131e5f2a8317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Auxiliary function to count parameters.\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Auxiliary function to print the number of parameters.\n",
    "def print_parameters(model):\n",
    "    print(\"Number of trainable parameters: \" + str(count_parameters(model)))\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad is True:\n",
    "            print(name + \" \" * (70 - len(name)) + \" -> Trained:\" + str(param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d3adb-32e7-42f9-ab08-fecbee9bb650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:07:16.435999Z",
     "iopub.status.busy": "2024-07-10T20:07:16.435609Z",
     "iopub.status.idle": "2024-07-10T20:07:16.439259Z",
     "shell.execute_reply": "2024-07-10T20:07:16.438520Z",
     "shell.execute_reply.started": "2024-07-10T20:07:16.435970Z"
    },
    "tags": []
   },
   "source": [
    "Lets count the number of trainable parameters currently in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f488440-8b9a-4b21-9a81-7895cb73829c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc26f14-2e7f-422d-9d8d-454dc254a66d",
   "metadata": {},
   "source": [
    "**87.8M parameters!!** And this is an \"small\" architecture compared with state-of-the-art models. Do we really need to finetune the whole model?\n",
    "\n",
    "I would say no! In this notebook we will learn how to avoid this challenge with PEFT :)\n",
    "\n",
    "**First, we will freeze all parameters in the backbone, but the classification head**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8194f5-befc-4ad0-86f3-2834f606b8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Freeze all parameters in backbone.\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False # Freeze.\n",
    "# Unfreeze classification head.\n",
    "for name, param in model[1].named_parameters():\n",
    "    param.requires_grad = True # Unfreeze.\n",
    "# Print trainable parameters\n",
    "print_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2cac4-9df1-4e24-9da7-3d12003a4fa7",
   "metadata": {},
   "source": [
    "Now, **we will explore different alternatives to selectively or additively adapt the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633543fa-1a43-415e-899d-022e78a3a430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:33:31.651317Z",
     "iopub.status.busy": "2024-07-10T20:33:31.650934Z",
     "iopub.status.idle": "2024-07-10T20:33:31.656310Z",
     "shell.execute_reply": "2024-07-10T20:33:31.655278Z",
     "shell.execute_reply.started": "2024-07-10T20:33:31.651288Z"
    },
    "tags": []
   },
   "source": [
    "------\n",
    "#### **1.5.1. Selective PEFT**\n",
    "\n",
    "These methods **tune only a small subset of the network**. \n",
    "- **Advantadges**: they usually not have specific hyper-parameters, and keep inference times.\n",
    "- **Drawbacks**: they might distort the pre-trained representations more severely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31de7a-564d-4fbe-9bfe-a6a090093742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:34:48.670522Z",
     "iopub.status.busy": "2024-07-10T20:34:48.670129Z",
     "iopub.status.idle": "2024-07-10T20:34:48.673962Z",
     "shell.execute_reply": "2024-07-10T20:34:48.673131Z",
     "shell.execute_reply.started": "2024-07-10T20:34:48.670496Z"
    },
    "tags": []
   },
   "source": [
    "#### **Affine-LN Tuning**\n",
    "\n",
    "**Tuning the Affine parameters from batch [1] or layer [2] normalization layers**, i.e. $\\gamma$ and $\\beta$. The intuiton behind the methos is: these parameters perform an scaling of such features more relevant for the task at hand, and decrease the scale of features unrelated to the downstream task.\n",
    "\n",
    "$$\\text{Affine} \\rightarrow  out=\\frac{x-E[x]}{\\sqrt{Var[x]+\\epsilon}}*\\gamma +\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d5973-e277-4a95-ba29-7233fad16794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a copy of the model to be adapted\n",
    "model_peft = copy.deepcopy(model).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca246cf-38f1-4277-b40b-839a9c5e220d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unfreeze encoder layer norm affine parameters... \", end=\"\\n\")\n",
    "for m in model_peft.modules():\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"layer_norm\" in name:        # Check parameters called layer norm.\n",
    "            param.requires_grad = True  # Set trainable to True.\n",
    "# Print trainable parameters\n",
    "print_parameters(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e35738-3c79-4834-91b2-b112f9530010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the selected parameters of the model.\n",
    "# Meanwhile, open a command window and check GPU usage with nvidia-smi.\n",
    "from vlms.utils import train_ft # Take a look to this function, for fine-tuning on mini-batches.\n",
    "# Define training hyper-parameters. Note that we decrease the number of epochs, since the convergence is faster.\n",
    "# with respect to the number of forward-backward passes. This is due to the more aggresive update.\n",
    "epochs, batch_size, learning_rate = 20, 16, 0.001\n",
    "# Set optimizer: we use Adam optimizer, which provides better convergence in deep architectures.\n",
    "optimizer = torch.optim.Adam(model_peft.parameters(), lr=learning_rate)\n",
    "train_ft(loader=train_loader, model=model_peft, optimizer=optimizer, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c1f98-fd46-4c9b-8cf9-a033e854cf1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict on test set - now, we will do it on mini-batches.\n",
    "from vlms.utils import predict # Take a look to this function! Inference on mini-batches.\n",
    "prob, Y_test = predict(test_loader, model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdb59a-d94d-4cf7-b259-f82524700631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825bc8a4-5a5e-4e7b-9565-bf5561afcf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T20:35:11.851986Z",
     "iopub.status.busy": "2024-07-10T20:35:11.851594Z",
     "iopub.status.idle": "2024-07-10T20:35:11.855174Z",
     "shell.execute_reply": "2024-07-10T20:35:11.854521Z",
     "shell.execute_reply.started": "2024-07-10T20:35:11.851959Z"
    },
    "tags": []
   },
   "source": [
    "#### **Bias Tuning**\n",
    "\n",
    "**Tuning only the Bias parameters** in ViTs, i.e. BitFit [3] has shown promising performance compared to full fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d03cf-0478-4391-8b4e-021d0f59e7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a copy of the model to be adapted.\n",
    "model_peft = copy.deepcopy(model).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce34bee-246c-447a-b226-9d373c8023e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Unfreeze bias parameters... \", end=\"\\n\")\n",
    "for m in model_peft.modules():\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"bias\" in name:              # Check parameters called bias.\n",
    "            param.requires_grad = True  # Set trainable to True.\n",
    "# Print trainable parameters\n",
    "print_parameters(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796e2c-3531-4ff7-a5aa-371eda95f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the selected parameters of the model.\n",
    "# Meanwhile, open a command window and check GPU usage with nvidia-smi.\n",
    "from vlms.utils import train_ft # Take a look to this function, for fine-tuning on mini-batches.\n",
    "# Define training hyper-parameters. Note that we decrease the number of epochs, since the convergence is faster.\n",
    "# with respect to the number of forward-backward passes. This is due to the more aggresive update.\n",
    "epochs, batch_size, learning_rate = 20, 16, 0.001\n",
    "# Set optimizer: we use Adam optimizer, which provides better convergence in deep architectures.\n",
    "optimizer = torch.optim.Adam(model_peft.parameters(), lr=learning_rate)\n",
    "train_ft(loader=train_loader, model=model_peft, optimizer=optimizer, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf8cce-f4c4-40c1-b90b-a1e51fb3b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set - now, we will do it on mini-batches\n",
    "from vlms.utils import predict # Take a look to this function! Inference on mini-batches\n",
    "prob, Y_test = predict(test_loader, model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b87beb-3d82-42b1-a14b-ab008951ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f208cc-68f6-4f1b-bfaa-e233e1352dd9",
   "metadata": {},
   "source": [
    "------\n",
    "#### **1.5.2. Additive PEFT**\n",
    "\n",
    "These methods add an small set of parameters, so-called Adapters. Usually, they perform residual modifications of pre-trained features. Nowadays, the most popular method is LoRA, which performs a low-rank adaptation.\n",
    " \n",
    "- **Advantadges**: they are more flexible than selective methods, since you can control how many parameters you introduce. The residual feature modification produces smoother changes.\n",
    "- **Drawbacks**: You need to set the number of parameters you introduce, which produce extra hyper-parameters. Some Adapters also increase inference time, since you are adding operations. Note that this is not neccesary the case of LoRA, since it can be computed in paralel.\n",
    "\n",
    "LoRA introduces to new matrices, A and B, which perform a residual modification on the output of an specific weight. Given a base linear weight $W$, and an input feature representation $x$, we can formalize LoRA as:\n",
    "\n",
    "$$out = W(x) + B(A(x))$$\n",
    "\n",
    "\n",
    "Where A and B are low-rank matrices.\n",
    "\n",
    "**How parameter-efficient are Low-rank Adapters?** Let us denote that $x$ has dimensionality of $D$ features, and $W$ is a Linear layer with $D$ features. If we were to use only one full-rank layer for the residual modification, which we denote as $W'$, such that $out = W(x) + W'(x)$, this new later would introduce $D\\times D$ parameters. Instead, the low-rank matrices A and B, with rank $r$ (e.g. $r=4$), have the dimensionality $A(D\\times r)$, and $B(r \\times D)$, such that A firstly compress the embedding in $r$ features, and B return it to the original dimensionality. Thus, the number of introduced parameters are $2 \\cdot r \\cdot D$. Image $D=128$, and typically, $r=4$. A basic Adapter would introduce 16.3K parameters, while LoRA introduces 1K.\n",
    "\n",
    "**Numbers apart, let's see how it works!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b955ed1-6dc1-4590-af04-baf565c24e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the LoRA layer, to replace any linear weight\n",
    "class _LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, w, w_a, w_b):\n",
    "        super().__init__()\n",
    "        self.w = w      # Original weight.\n",
    "        self.w_a = w_a  # Matrix A.\n",
    "        self.w_b = w_b  # Matrix B.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w(x) + self.w_b(self.w_a(x)) # Residual modification with Adapter.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40e3fb-be5d-4789-a133-b08898075a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create LoRA Wrapper\n",
    "class LoRAWrapper(torch.nn.Module):\n",
    "    def __init__(self, vit_model, r=4):\n",
    "        super(LoRAWrapper, self).__init__()\n",
    "        # Inits\n",
    "        self.ViTbase = vit_model # ViT to modify\n",
    "        self.r = r               # Rank\n",
    "        # create for storage, then we can init them or load weights.\n",
    "        self.w_As = []  # Storage for linear layers of A matrices.\n",
    "        self.w_Bs = []  # Storage for linear layers of B matrices.\n",
    "        \n",
    "        # We go trough the base encoder, detect Multi-Head Attention blocks, and modify adding the Adapters.\n",
    "        for i, layer in enumerate(list(list(self.ViTbase.encoder.children())[2].modules())):\n",
    "            if layer._get_name() == 'CLIPAttention':  # Multi-Head Attention Blocks.\n",
    "\n",
    "                # k_proj (key)\n",
    "                w_a_linear_qkv = torch.nn.Linear(layer.k_proj.in_features, r, bias=False) # layer for A matrix.\n",
    "                w_b_linear_qkv = torch.nn.Linear(r, layer.k_proj.in_features, bias=False) # layer for B matrix.\n",
    "                torch.nn.init.zeros_(w_b_linear_qkv.weight)                               # Set values in B to 0s.\n",
    "                self.w_As.append(w_a_linear_qkv), self.w_Bs.append(w_b_linear_qkv)        # Store new weights.\n",
    "                layer.k_proj = _LoRALayer(layer.k_proj, w_a_linear_qkv, w_b_linear_qkv)   # Modify layer with LoRA layer.\n",
    "\n",
    "                # v_proj (query)\n",
    "                w_a_linear_qkv = torch.nn.Linear(layer.v_proj.in_features, r, bias=False) # layer for A matrix.\n",
    "                w_b_linear_qkv = torch.nn.Linear(r, layer.v_proj.in_features, bias=False) # layer for B matrix.\n",
    "                torch.nn.init.zeros_(w_b_linear_qkv.weight)                               # Set values in B to 0s.\n",
    "                self.w_As.append(w_a_linear_qkv), self.w_Bs.append(w_b_linear_qkv)        # Store new weights.\n",
    "                layer.v_proj = _LoRALayer(layer.v_proj, w_a_linear_qkv, w_b_linear_qkv)   # Modify layer with LoRA layer.\n",
    "\n",
    "                # q_proj (value)\n",
    "                w_a_linear_qkv = torch.nn.Linear(layer.q_proj.in_features, r, bias=False) # layer for A matrix.\n",
    "                w_b_linear_qkv = torch.nn.Linear(r, layer.q_proj.in_features, bias=False) # layer for B matrix.\n",
    "                torch.nn.init.zeros_(w_b_linear_qkv.weight)                               # Set values in B to 0s.\n",
    "                self.w_As.append(w_a_linear_qkv), self.w_Bs.append(w_b_linear_qkv)        # Store new weights.\n",
    "                layer.q_proj = _LoRALayer(layer.q_proj, w_a_linear_qkv, w_b_linear_qkv)   # Modify layer with LoRA layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ViTbase(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303cdd0-312a-4957-8faf-b6186eddb05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a copy of the model to be adapted.\n",
    "model_peft = copy.deepcopy(model).eval().to(device)\n",
    "# Add LoRA Wrapper to the model.\n",
    "r=4                                                        # Rank for low-rank adaptation. This is a hyper-parameter.\n",
    "model_peft[0] = LoRAWrapper(model_peft[0], r=r).to(device) # Modify vision backbone with the new architecture with Adapters\n",
    "# Print trainable parameters\n",
    "print_parameters(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb66fc-7108-4483-a58a-46a26375a81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the selected parameters of the model.\n",
    "# Meanwhile, open a command window and check GPU usage with nvidia-smi.\n",
    "from vlms.utils import train_ft # Take a look to this function, for fine-tuning on mini-batches.\n",
    "# Define training hyper-parameters. Note that we decrease the number of epochs, since the convergence is faster.\n",
    "# with respect to the number of forward-backward passes. This is due to the more aggresive update.\n",
    "epochs, batch_size, learning_rate = 20, 16, 0.001\n",
    "# Set optimizer: we use Adam optimizer, which provides better convergence in deep architectures.\n",
    "optimizer = torch.optim.Adam(model_peft.parameters(), lr=learning_rate)\n",
    "train_ft(loader=train_loader, model=model_peft, optimizer=optimizer, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb90265-4bcd-4124-aeae-b5ec085e32a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict on test set - now, we will do it on mini-batches.\n",
    "from vlms.utils import predict # Take a look to this function! Inference on mini-batches.\n",
    "prob, Y_test = predict(test_loader, model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd98d18-4a06-4e8f-8907-4dd96008c6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478199e8-9f93-4b41-bd77-654161b24ec0",
   "metadata": {},
   "source": [
    "--- \n",
    "##### **ACTIVITY**\n",
    "\n",
    "Well, now you know the basics of PEFT methods. If you want to know more, I reccomend:\n",
    "\n",
    "- How is the performance if you do not initialize the B matrix with 0s in LoRA? How rank modification in LoRA affects the performance?\n",
    "- Doing early stopping based on validation data also helps on avoiding over-fitting during PEFT. Modify the loaders function to create a few-shot dataset for validation, and modify training to save the best model based on validation loss.\n",
    "- Explore the comparison with Black-box Adapters for more and less than 16 shots.\n",
    "- Try developing the same pipeline for [CONCH](https://huggingface.co/MahmoodLab/CONCH) [4], a revently introduced VLM for histology. Its vision backbone is large scale (ViT-B/16), takes large-resolution input images (448x448), and is pre-trained with more data. How does model scaling translates to PEFT? As you can see, the larger the network, the more convinient is PEFT with respet to full fine-tuning.\n",
    "\n",
    "\n",
    "--- \n",
    "## **References**\n",
    "\n",
    "\n",
    "[1] Frankle, J., Schwab, D. J., Morcos, A. S. (2021). Training batchnorm and only batchnorm: On the expressive power of random features in cnns. International Conference on Learning Representations (ICLR). \\\n",
    "[2] Ben-Zaken, E., Ravfogel, S., Goldberg, Y. (2021). Bitfit: Simple parameter efficient fine-tuning for transformer-based masked language-models. Association for Computational Linguistics. \\\n",
    "[3] Hu, E. J., et al., (2022). LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR). \\\n",
    "[4] Lu, M.Y., Chen, B., Williamson, D.F.K. et al. (2024) A visual-language foundation model for computational pathology. Nature Medicine.\n",
    "\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
