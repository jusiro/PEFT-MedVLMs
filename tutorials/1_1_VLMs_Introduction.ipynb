{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83387cdb-a34a-47c0-9cf6-0c1a4ef7736b",
   "metadata": {},
   "source": [
    "### Vision-Language Medical Foundation Models\n",
    "\n",
    "#### 1.1. Introduction, application, VLMs, and Transformers library\n",
    "---\n",
    "\n",
    "In this tutorial, we will explore the use of **vision-language models for medical image analysis (medVLMs)**. In particular, we will focus on:\n",
    "\n",
    "    - Contrastive Image-Text pre-training (CLIP)\n",
    "    - Zero-shot classification\n",
    "        - Single prompt\n",
    "        - Prompt ensemble\n",
    "    - Few-shot bla\n",
    "        - Linear Probing\n",
    "        - CLIP-Adapter\n",
    "        - Advance Linear Probing techniques\n",
    "    - Parameter-Efficient Fine-TUning\n",
    "        - Selective methods\n",
    "        - Additive methods\n",
    "    \n",
    "More concretely, we will explore examples using foundation models specialized on **histology images** (plip [1] /conch [2]). Nevertheless, note that the introduced methodologies are **applicable to ***any*** medVLM**. \n",
    "\n",
    "In this tutorial, we will build upon [huggingface](https://huggingface.co/), using the library `transformers`. This library is becoming increasingly popular, and contains an intersting number of tutoriasls/examples. I recommend you to take a look!\n",
    "\n",
    "Said that, let's start!\n",
    "\n",
    "First, in this notebook, **we will introduce the application addressed, the main vision-language foundation model employed, and an introduction to Transformers library**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06843f9-0c6d-4eb5-8ffb-8b41cc01b00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device for training/inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Available device: \" + device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bf7e3-9628-4021-a876-fb8b74088b36",
   "metadata": {},
   "source": [
    "#### **Preliminaries: Gleason grading application**\n",
    "In this notebook, we will explore histology image analysis, and in particular, prostate cancer grading. To do so, we will use SICAPv2 dataset [3]. This dataset contains tissue patches labeled by expert pathologists according to its cancer severity: non-cancerous (NC), Gleason grade 3, (G3), GLeason grade 4 (G4), and grade 5 (G5). This labels are directly correlacted with patient prognosis, and measure the grade of differentiation of the glands in the tussuie, i.e. less diferentation implies worst prognosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e2374-aad8-4864-bb63-9cf27f9b3e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fist, lets visualize few examples for each category.\n",
    "nc = Image.open(\"./local_data/datasets/SICAPv2/images/16B0028148_Block_Region_8_2_14_xini_27858_yini_55555.jpg\") \n",
    "g3 = Image.open(\"./local_data/datasets/SICAPv2/images/18B0006169B_Block_Region_6_3_7_xini_33958_yini_90365.jpg\") \n",
    "g4 = Image.open(\"./local_data/datasets/SICAPv2/images/17B0032153_Block_Region_10_13_17_xini_23105_yini_15687.jpg\") \n",
    "g5 = Image.open(\"./local_data/datasets/SICAPv2/images/16B0008067_Block_Region_0_6_2_xini_10859_yini_103113.jpg\") \n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(nc)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Non-cancerous\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(g3)\n",
    "plt.title(\"Gleason grade 3\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(g4)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Gleason grade 4\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(g5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Gleason grade 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3278fe0-7ce5-4f50-b2f8-31d43f3745f9",
   "metadata": {},
   "source": [
    "The objective is perform a multi-class prediction to automatically grade such images, leveraging foundation models. We will employ the train/test splits provided in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766f9da-f6a6-4d93-87ac-9d3f2d84778e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SICAPv2 dataset metadata.\n",
    "categories = [\"NC\", \"G3\", \"G4\", \"G5\"]                                        # List of categories.\n",
    "path_images = \"./local_data/datasets/SICAPv2/images/\"                        # Folder with the images.\n",
    "dataframe_train = \"./local_data/datasets/SICAPv2/partition/Test/Train.xlsx\"  # Dataframe (Table) containing train images names and labels.\n",
    "dataframe_test = \"./local_data/datasets/SICAPv2/partition/Test/Test.xlsx\"    # Dataframe (Table) containing test images names and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80696df8-d526-4238-9aeb-4ab897014a74",
   "metadata": {},
   "source": [
    "#### **Preliminaries: PLIP, Transformers library and VLMs structure**\n",
    "\n",
    "We will use in out main experiments **PLIP**, a **vision-language model specialized on histology image**. Interestingly, this model was pre-trained using Twitter data, by leveraging pathologist's comments on shared cases. You can now more [here](https://www.nature.com/articles/s41591-023-02504-3) [2], or take a look to its dema [here](https://huggingface.co/spaces/vinid/webplip). The architecture follows one of the CLIP options, using **ViT-B/32 and vision encoder, and GPT as text encoder, which are fine-tuned on histology data**.\n",
    "\n",
    "First, we will start by digging-in the library **Transformers** organization, and how vision-language models are usually implemented there. Even though this library is quite useful to fastly access pre-trained models and perform inference, **we will need to make some adjustments if we want to do some more advance trainings/adaptations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b05b46-d825-48f8-9903-afe0d2b48961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and pre-processing tools from huggingface.\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# In Transformers library, models and versions are storages by an ID defininf the use and model name.\n",
    "# For PLIP model, such ID is \"vinid/plip\".\n",
    "processor = AutoProcessor.from_pretrained(\"vinid/plip\") # pre-processing image and text.\n",
    "plip = AutoModel.from_pretrained(\"vinid/plip\").eval()   # model with pre-trained weights.\n",
    "# We set model in eval mode to avoid droput inference and batchnorm stats update in CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3af7e-c53f-4c91-b60e-8aec559bb3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, les'ts inspect the pre-processing operations.\n",
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef8fd1-a1c3-4608-af73-b791ccdd13f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can see that the Image Processor contains a set of operations such as image resizing,\n",
    "# intensity normalization, and its requried average and std values. We will only do an small change:\n",
    "# remove the option \"do_center_crop\", since we want to keep the entire image.\n",
    "processor.image_processor.do_center_crop = False\n",
    "\n",
    "# The text pre-processing class contains a tokenizer to prepare string inputs into a numerical \n",
    "# strucute that we can feed into the text encoder. So far, we will leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0432390-56c6-4a49-ae73-2d591aa5e833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLIP model is not in a very friendly format for using it. Since we want to operate separately\n",
    "# with the vision and text encoders, we will separate both backbones, and incorporate their\n",
    "# projections into an l2-norm space. \n",
    "\n",
    "class PLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, proj_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder         # Take one-modality encoder from VLM.\n",
    "        self.proj_layer = proj_layer   # Take projection layer into joint embedding space.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Forward input trough encoder\n",
    "        features = self.encoder(**inputs).pooler_output # Forward trough encoder - we keep a global feature for the image/text\n",
    "        \n",
    "        # Project features\n",
    "        projected_features = self.proj_layer(features)  # Apply projection\n",
    "        \n",
    "        # Ensure image features are l2-norm\n",
    "        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True) # l2-normalization\n",
    "\n",
    "        return projected_features\n",
    "\n",
    "# Create model wrapper for vision and text encoders\n",
    "vision_encoder = PLIPWrapper(plip.vision_model, plip.visual_projection)\n",
    "text_encoder = PLIPWrapper(plip.text_model, plip.text_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0edd2-565e-4f74-8db5-7cb81e6b1928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets inspect the outputs on one image\n",
    "\n",
    "# Read an image (G5) and convert to array\n",
    "im = Image.open(\"./local_data/datasets/SICAPv2/images/16B0008067_Block_Region_0_6_2_xini_10859_yini_103113.jpg\") \n",
    "im = np.array(im)\n",
    "print(\"Raw image shape:\")\n",
    "print(str(list(im.shape)))\n",
    "print(\"\", end=\"\\n\")\n",
    "\n",
    "# Pre-processs the image, i.e.: resize, channel transpose, intensity normalization, etc.\n",
    "inputs = processor.image_processor(im, return_tensors=\"pt\")\n",
    "# The \"inputs\" in this case are pixel values of input image\n",
    "print(\"Analyzing the input image.\")\n",
    "print(\"   Elements after pre-processing:\")\n",
    "print(\"   \" + str(inputs.keys()))\n",
    "print(\"   Pre-processed image shape:\")\n",
    "print(\"   \" + str(list(inputs['pixel_values'].shape)))\n",
    "print(\"\", end=\"\\n\")\n",
    "\n",
    "# Forward image trough vision encoder\n",
    "with torch.no_grad():\n",
    "    vision_features = vision_encoder(inputs)\n",
    "\n",
    "# Let's check the characteristic of the feature representation\n",
    "print(\"Analyzing the output of the vision encoder.\")\n",
    "print(\"   Vision embedding shape:\")\n",
    "print(\"   \" + str(list(vision_features.shape)))\n",
    "print(\"   Norm of the vector:\")\n",
    "print(\"   \" + str(vision_features.norm(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76186d8b-dd32-4ecb-a5e5-5aeb5888edd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets inspect the outputs on one text prompt\n",
    "\n",
    "# Define text input\n",
    "prompt = [\"Healthy prostate tissue\",\n",
    "          \"Non cancerous prostate tissue\"\n",
    "          \"a high resolution medical image\",\n",
    "          \"Gleason grade 4\",\n",
    "          \"histology tissue with ill-defined glands\",\n",
    "          \"Gleason grade 5\"]\n",
    "\n",
    "# Tokenize text (padding will help to work over batches of texts in forward pass)\n",
    "inputs = processor.tokenizer(prompt, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "\n",
    "# Inspect inputs\n",
    "print(\"Analyzing the input image.\")\n",
    "print(\"   Elements after pre-processing:\")\n",
    "print(\"   \" + str(inputs.keys())) # Check to know more about the keys: https://lukesalamone.github.io/posts/what-are-attention-masks/\n",
    "print(\"   Input embeddings:\")\n",
    "print(\"   \" + str(list(inputs['input_ids'].shape)))\n",
    "print(\"   Attention masks:\")\n",
    "print(\"   \" + str(list(inputs['input_ids'].shape)))\n",
    "print(\"\", end=\"\\n\")\n",
    "\n",
    "# Forward image trough text encoder\n",
    "with torch.no_grad():\n",
    "    text_features = text_encoder(inputs)\n",
    "\n",
    "# Let's check the characteristic of the feature representation\n",
    "print(\"Analyzing the output of the text encoder.\")\n",
    "print(\"   Text embedding shape:\")\n",
    "print(\"   \" + str(list(text_features.shape)))\n",
    "print(\"   Norm of the vector:\")\n",
    "print(\"   \" + str(text_features.norm(dim=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30918920-1c85-4d2b-a781-03565b7ab166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finally, we can compute similarities between image and text embeddings\n",
    "# using dot product. Remember, since they are l2-normalized, such similarities\n",
    "# are equivalent to computing the cosine similarity.\n",
    "\n",
    "# We multiply by the pre-trained temperature scaling, which calibrated the\n",
    "# similarity considered \"high\" or \"low\" (e.g. a cosine similarity of 0.2)\n",
    "# could already mean that both imports are semantically similar.\n",
    "\n",
    "# Cosine similarities\n",
    "with torch.no_grad():\n",
    "    sim = vision_features @ text_features.t()\n",
    "print(\"Cosine similarities: \" + str(sim))\n",
    "\n",
    "# Tempeature-calibrated similarities\n",
    "with torch.no_grad():\n",
    "    sim = vision_features @ text_features.t() * plip.logit_scale.exp().item()\n",
    "print(\"logits: \" + str(sim))\n",
    "\n",
    "# Softmax outputs\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(vision_features @ text_features.t() * plip.logit_scale.exp(), axis=-1)\n",
    "print(\"softmax: \" + str(prob))\n",
    "\n",
    "# Index of predicted category\n",
    "print(\"Predicted category: \" + prompt[torch.argmax(prob, -1).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d161082-a796-4ace-99f8-dcc4fbc36420",
   "metadata": {},
   "source": [
    "Now that we are familiar with the structure of a VLM, and the pre-processing tools they require for each modality, we will move to make a toy example on how these models are pre-trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc168c-d2f4-4aa4-bd69-36725c43872b",
   "metadata": {},
   "source": [
    "--- \n",
    "## **References**\n",
    "\n",
    "[1] Silva-Rodr√≠guez, J., Colomer, A., Sales, M. A., Molina, R., & Naranjo, V. (2020). Going deeper through the Gleason scoring scale : An automatic end-to-end system for histology prostate grading and cribriform pattern detection. Computer Methods and Programs in Biomedicine. \\\n",
    "[2] Huang Z, Bianchi F, Yuksekgonul M, Montine TJ, Zou J. (2023). A visual-language foundation model for pathology image analysis using medical Twitter. Nature Medicine. \\\n",
    "[3] Lu, M.Y., Chen, B., Williamson, D.F.K. et al. (2024) A visual-language foundation model for computational pathology. Nature Medicine.\n",
    "\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
