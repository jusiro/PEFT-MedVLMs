{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0fb891-60c0-4cca-9e53-69e0c19aef61",
   "metadata": {},
   "source": [
    "### **Vision-Language Medical Foundation Models**\n",
    "\n",
    "#### **1.3. Zero-shot classification**\n",
    "---\n",
    "\n",
    "Pre-trained vision-language models are capable or performing the so-called **zero-shot predictions**. These image-level predictions are driven by the language encoder, thanks to the multi-modal alignment. **Given a set of descriptions for a subset of target categories, we can compute text prototypes of each category** in the common space. Given a new image, **the class assigned will be the one corresponding to the most similar text prototype** to the image embedding.\n",
    "\n",
    "In this notebook, we will explore two popular forms to perform zero-shot predictions: single prompt, and prompt ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45622a-cab1-4071-bd91-00de72eff7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device for training/inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Available device: \" + device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65561bf4-8245-4932-b34f-4e8814cc8a5d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-10T00:30:14.390545Z",
     "iopub.status.idle": "2024-07-10T00:30:14.391149Z",
     "shell.execute_reply": "2024-07-10T00:30:14.390995Z",
     "shell.execute_reply.started": "2024-07-10T00:30:14.390976Z"
    },
    "tags": []
   },
   "source": [
    "#### **Dataset details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cbee3-9d59-4d0a-97b2-5112d2ecb3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SICAPv2 dataset metadata\n",
    "categories = [\"NC\", \"G3\", \"G4\", \"G5\"]                                        # List of categories\n",
    "path_images = \"./local_data/datasets/SICAPv2/images/\"                        # Folder with the images\n",
    "dataframe_train = \"./local_data/datasets/SICAPv2/partition/Test/Train.xlsx\"  # Dataframe (Table) containing train images names and labels\n",
    "dataframe_test = \"./local_data/datasets/SICAPv2/partition/Test/Test.xlsx\"    # Dataframe (Table) containing test images names and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ff59a-1930-4a6a-ba37-918d8a00a1b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T00:30:55.502043Z",
     "iopub.status.busy": "2024-07-10T00:30:55.501643Z",
     "iopub.status.idle": "2024-07-10T00:30:55.505159Z",
     "shell.execute_reply": "2024-07-10T00:30:55.504471Z",
     "shell.execute_reply.started": "2024-07-10T00:30:55.502015Z"
    },
    "tags": []
   },
   "source": [
    "#### **Load model, pre-processing, and wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc2e9f-affa-4961-aa79-7537dfabf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and pre-processing tools from huggingface\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# In Transformers library, models and versions are storages by an ID defininf the use and model name.\n",
    "# For PLIP model, such ID is \"vinid/plip\"\n",
    "processor = AutoProcessor.from_pretrained(\"vinid/plip\") # pre-processing image and text\n",
    "processor.image_processor.do_center_crop = False\n",
    "plip = AutoModel.from_pretrained(\"vinid/plip\").eval() # model with pre-trained weights\n",
    "# We set model in eval mode to avoid droput inference and batchnorm stats update in CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac9dee-10db-4c05-9f21-a5e2105d4f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, we will use our PLIP Wrapper for easy interaction\n",
    "class PLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, proj_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder         # Take one-modality encoder from VLM.\n",
    "        self.proj_layer = proj_layer   # Take projection layer into joint embedding space.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Forward input trough encoder\n",
    "        features = self.encoder(**inputs).pooler_output # Forward trough encoder - we keep a global feature for the image/text\n",
    "        \n",
    "        # Project features\n",
    "        projected_features = self.proj_layer(features)  # Apply projection\n",
    "        \n",
    "        # Ensure image features are l2-norm\n",
    "        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True) # l2-normalization\n",
    "\n",
    "        return projected_features\n",
    "\n",
    "# Create model wrapper for vision and text encoders\n",
    "vision_encoder = PLIPWrapper(plip.vision_model, plip.visual_projection)\n",
    "text_encoder = PLIPWrapper(plip.text_model, plip.text_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe88a07-40c8-47cf-9df4-8f7ee4895f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:42:34.659497Z",
     "iopub.status.busy": "2024-07-08T15:42:34.659094Z",
     "iopub.status.idle": "2024-07-08T15:42:34.662920Z",
     "shell.execute_reply": "2024-07-08T15:42:34.662034Z",
     "shell.execute_reply.started": "2024-07-08T15:42:34.659467Z"
    },
    "tags": []
   },
   "source": [
    "#### **Feature extraction**\n",
    "First, we need to extract all feature representations from the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b0947-e194-4b6c-8140-6a1c6d0aefc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To run over the whole dataset, we move now the vision model to gpu\n",
    "vision_encoder.to(device)\n",
    "\n",
    "# We need to format the pre-processing transforms in a more friendly format for torch Dataloaders\n",
    "from torchvision import transforms\n",
    "\n",
    "# Pre-processing transforms to apply during data loading.\n",
    "plip_transforms = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),                                                 # Move PIL/array image to tensor\n",
    "    transforms.Normalize(std=processor.image_processor.image_std,\n",
    "                         mean=processor.image_processor.image_mean),       # Intensity normalization\n",
    "    transforms.Resize(list(processor.image_processor.crop_size.values()))  # Resize to pre-trained resolution\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa197d2e-0f20-4776-9702-199390cc525b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloader with the whole testing subset\n",
    "from vlms.data import loader # Take a look to this function to create the data loader\n",
    "test_loader = loader(dataframe_path=dataframe_test, path_images=path_images, categories=categories,\n",
    "                     transforms=plip_transforms, batch_size=8, num_workers=0)\n",
    "\n",
    "# We can check the dataset format and available samples\n",
    "print(\"Samples available for testing: \" + str(len(test_loader.dataset.data)))\n",
    "print(test_loader.dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8099216-072e-4a3f-812d-79212bfaa717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "from vlms.utils import extract_features\n",
    "X_test, Y_test = extract_features(test_loader, vision_encoder)\n",
    "\n",
    "# Lets check the training dataset\n",
    "print(\"Test features: \" + str(X_test.shape))\n",
    "print(\"Test labels: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6a7a-8b23-4b3e-a9c7-46313788d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:40:45.463612Z",
     "iopub.status.busy": "2024-07-08T15:40:45.463236Z",
     "iopub.status.idle": "2024-07-08T15:40:45.466830Z",
     "shell.execute_reply": "2024-07-08T15:40:45.466202Z",
     "shell.execute_reply.started": "2024-07-08T15:40:45.463582Z"
    },
    "tags": []
   },
   "source": [
    "#### **1.3.1. Single prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac1a39-37c0-4bfb-8304-79383ed4f81b",
   "metadata": {},
   "source": [
    "One the vision-language is pre-trained, **vision and image feature spaces are aligned**. This is, a **text description of a concept should produce a similar representation in the shared embedding space than an image of such category**. This phenomenon is profited to perform **zero-shot prediction without any adaptation** of the VLM. **Class-wise embeddings (class prototypes)** are computed from text descriptions of each category. Such prototypes for C categories and Dt features can be embeded into a feature matrix W. **Note that his is similar to a Linear layer in a classical MLP output (without bias term)**. Thus, **class predictions (logits) can be computed fron vision features Fv by performing matrix multiplication**, as in a fully-connected layer: Fv x transpose(W) = out -> (1xDv) x (DvxC) = (1xC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0d5ef-07e8-4170-8577-d2c8d8f37315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define text input\n",
    "class_prompts = [\"non-cancerous\",\n",
    "                 \"Gleason grade 3\",\n",
    "                 \"Gleason grade 4\",\n",
    "                 \"Gleason grade 5\"]\n",
    "\n",
    "# Tokenize text\n",
    "inputs = processor.tokenizer(class_prompts, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "\n",
    "# Compute text protoypes per class\n",
    "with torch.no_grad():\n",
    "    class_prototypes = text_encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74c830-59e5-4cf3-8f9f-067ddc44c6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(torch.tensor(X_test) @ class_prototypes.t() * plip.logit_scale.exp(), axis=-1)\n",
    "    prob = prob.detach().numpy()\n",
    "    \n",
    "print(\"Prediction shape: \" + str(prob.shape))\n",
    "print(\"Example: \" + str(prob[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e4c1-a5d1-4bdc-bb02-114ed14ea033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64629169-f4cd-4a80-b150-239b306b6bb9",
   "metadata": {},
   "source": [
    "#### **1.3.2. Prompt ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfdf82-be2b-4b04-82b8-47f6c5fc4217",
   "metadata": {},
   "source": [
    "A popular option to refine the text prototype, is to **combine multiple prompts (prompt ensemble)**, which are averaged per class. Thus, **noisy features in an specific prompt are alleviated**, and usually, performance is improved. Such prompt ensemble comes usually from using **different templates**, i.e. \"A photo of [CLS]\", and **different descriptions** of the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655bcf3-f7a4-497f-b6fc-dd9df976648d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble of templates\n",
    "templates = [\"a histopathology slide showing [CLS]\", \"histopathology image of [CLS]\",\n",
    "             \"pathology tissue showing [CLS]\", \"presence of [CLS] tissue on image\"]\n",
    "\n",
    "# Category-wise descriptions, which are more informative than category names. For instance, \"atrophic dense glands\" better \n",
    "# describes the local findings associated with Gleason grade 3.\n",
    "prompts_dict = {\"NC\": [\"benign glands\"],\n",
    "                \"G3\": [\"atrophic dense glands\"],\n",
    "                \"G4\": [\"cribriform ill-formed fused papillary patterns\"],\n",
    "                \"G5\": [\"isolated nest cells without lumen roseting patterns\"]}\n",
    "\n",
    "# Combine all paired options of templates and descriptions\n",
    "prompts = {}\n",
    "for iCategory in categories:\n",
    "    prompts[iCategory] = [caption.replace(\"[CLS]\", iDescription) for iDescription in prompts_dict[iCategory]\n",
    "                          for caption in templates]\n",
    "\n",
    "# Compute embeddings per category\n",
    "class_prototypes = []\n",
    "for iKey in range(len(categories)):\n",
    "    with torch.no_grad():\n",
    "        # Retrieve descriptions for that particular category\n",
    "        descriptions = prompts[categories[iKey]]\n",
    "        # Tokenize text\n",
    "        inputs = processor.tokenizer(descriptions, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "        # Forward text encoder\n",
    "        text_features_ensemble = text_encoder(inputs)\n",
    "        # Get class prototypes as average of all text prompts\n",
    "        avg_text_features = text_features_ensemble.mean(0).unsqueeze(0)\n",
    "        # Re-normalize embedding\n",
    "        avg_text_features = avg_text_features / avg_text_features.norm(dim=-1, keepdim=True)\n",
    "        class_prototypes.append(avg_text_features)\n",
    "                               \n",
    "# Concatenate all class prototypes\n",
    "class_prototypes = torch.concat(class_prototypes, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289585ba-aab4-47e6-9643-68355fc0f3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(torch.tensor(X_test) @ class_prototypes.t() * plip.logit_scale.exp(), axis=-1)\n",
    "    prob = prob.detach().numpy()\n",
    "    \n",
    "print(\"Prediction shape: \" + str(prob.shape))\n",
    "print(\"Example: \" + str(prob[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3b4a0-c5c9-44d2-8d85-721aa687a615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))\n",
    "# As you can see, prompt ensemble boost the performance! +6.7% balanced accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756dbe6c-bcb6-4c3c-851b-d4ff7cc16182",
   "metadata": {},
   "source": [
    "--- \n",
    "##### **ACTIVITY**\n",
    "\n",
    "Well, now you know everything you need about zero-shot predictions. If you want to know more, I reccomend:\n",
    "\n",
    "- Try developing the same pipeline for [CONCH](https://huggingface.co/MahmoodLab/CONCH) [1], a revently introduced VLM for histology. Its vision backbone is large scale, takes large-resolution input images, and is pre-trained with more data. How does model scaling translates to black-box Adaptation?\n",
    "- Indeed, CONCH also uses a different subset of templates for zero-shot in Gleason grades (which I share below). How the prompt selection affect performance? How realistic is designing prompts to optimize test performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b791b-07a8-48d2-b8cd-5c13b32fa2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text ensemble for CONCH\n",
    "templates = [\"[CLS].\",\n",
    "            \"a photomicrograph showing [CLS].\",\n",
    "            \"a photomicrograph of [CLS].\",\n",
    "            \"an image of [CLS].\",\n",
    "            \"an image showing [CLS].\",\n",
    "            \"an example of [CLS].\",\n",
    "            \"[CLS] is shown.\",\n",
    "            \"this is [CLS].\",\n",
    "            \"there is [CLS].\",\n",
    "            \"a histopathological image showing [CLS].\",\n",
    "            \"a histopathological image of [CLS].\",\n",
    "            \"a histopathological photograph of [CLS].\",\n",
    "            \"a histopathological photograph showing [CLS].\",\n",
    "            \"shows [CLS].\",\n",
    "            \"presence of [CLS].\",\n",
    "            \"[CLS] is present.\",\n",
    "            \"an H&E stained image of [CLS].\",\n",
    "            \"an H&E stained image showing [CLS].\",\n",
    "            \"an H&E image showing [CLS].\",\n",
    "            \"an H&E image of [CLS].\",\n",
    "            \"[CLS], H&E stain.\",\n",
    "            \"[CLS], H&E.\"]\n",
    "\n",
    "prompts_dict = {\"NC\": [\"non-cancerous tissue\", \"non-cancerous prostate tissue\", \"benign tissue\", \"benign glands\", \n",
    "                       \"benign prostate glands\", \"benign prostate tissue\"],\n",
    "                \"G3\": [\"gleason grade 3\", \"gleason pattern 3\", \"prostate cancer, gleason grade 3\", \n",
    "                       \"prostate cancer, gleason pattern 3\", \"prostate adenocarcinoma, well-differentiated\",\n",
    "                       \"well-differentiated prostatic adenocarcinoma\"],\n",
    "                \"G4\": [\"gleason grade 4\", \"gleason pattern 4\", \"prostate cancer, gleason grade 4\", \n",
    "                       \"prostate cancer, gleason pattern 4\", \"prostate adenocarcinoma, moderately differentiated\",  \n",
    "                       \"moderately differentiated prostatic adenocarcinoma\"],\n",
    "                \"G5\": [\"gleason grade 5\", \"gleason pattern 5\", \"prostate cancer, gleason grade 5\",\n",
    "                       \"prostate cancer, gleason pattern 5\", \"prostate adenocarcinoma, poorly differentiated\",\n",
    "                       \"poorly differentiated prostatic adenocarcinoma\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5ff4-645d-46b6-93f9-3f06b4192543",
   "metadata": {},
   "source": [
    "--- \n",
    "## **References**\n",
    "\n",
    "[1] Lu, M.Y., Chen, B., Williamson, D.F.K. et al. (2024) A visual-language foundation model for computational pathology. Nature Medicine.\n",
    "\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
