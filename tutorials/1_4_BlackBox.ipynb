{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0fb891-60c0-4cca-9e53-69e0c19aef61",
   "metadata": {},
   "source": [
    "### **Vision-Language Medical Foundation Models**\n",
    "\n",
    "#### **1.4. Few-shot black-box Adapters**\n",
    "---\n",
    "\n",
    "**Objective**: Given an small set of examples per category, we want to efficiently use the vision features to perform classification on a downstream task, without fine-tuning the base model.\n",
    "\n",
    "**Few-shot**: We only use K number of images for each new category.\n",
    "\n",
    "**Why black-box Adapters?**: They are efficient, usually run over CPU. They are fast: you can transfer the model in a matter of minutes. They are backbone-agnostic, this is, they work the same over any vision encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45622a-cab1-4071-bd91-00de72eff7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device for training/inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Available device: \" + device)\n",
    "\n",
    "# Seeds for reproducibility\n",
    "def set_seeds(seed_value, use_cuda):\n",
    "    np.random.seed(seed_value)     # cpu vars\n",
    "    torch.manual_seed(seed_value)  # cpu  vars\n",
    "    random.seed(seed_value)        # Python\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)     # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  # needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(42, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65561bf4-8245-4932-b34f-4e8814cc8a5d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-10T00:30:14.390545Z",
     "iopub.status.idle": "2024-07-10T00:30:14.391149Z",
     "shell.execute_reply": "2024-07-10T00:30:14.390995Z",
     "shell.execute_reply.started": "2024-07-10T00:30:14.390976Z"
    },
    "tags": []
   },
   "source": [
    "#### **Dataset details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cbee3-9d59-4d0a-97b2-5112d2ecb3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SICAPv2 dataset metadata\n",
    "categories = [\"NC\", \"G3\", \"G4\", \"G5\"]                                        # List of categories\n",
    "path_images = \"./local_data/datasets/SICAPv2/images/\"                        # Folder with the images\n",
    "dataframe_train = \"./local_data/datasets/SICAPv2/partition/Test/Train.xlsx\"  # Dataframe (Table) containing train images names and labels\n",
    "dataframe_test = \"./local_data/datasets/SICAPv2/partition/Test/Test.xlsx\"    # Dataframe (Table) containing test images names and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae44b21-8c2b-4f78-9cbd-0b713a18eb26",
   "metadata": {},
   "source": [
    "#### **VLM model wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc2e9f-affa-4961-aa79-7537dfabf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and pre-processing tools from huggingface\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# In Transformers library, models and versions are storages by an ID defininf the use and model name.\n",
    "# For PLIP model, such ID is \"vinid/plip\"\n",
    "processor = AutoProcessor.from_pretrained(\"vinid/plip\") # pre-processing image and text\n",
    "processor.image_processor.do_center_crop = False\n",
    "plip = AutoModel.from_pretrained(\"vinid/plip\").eval() # model with pre-trained weights\n",
    "# We set model in eval mode to avoid droput inference and batchnorm stats update in CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac9dee-10db-4c05-9f21-a5e2105d4f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, we will use our PLIP Wrapper for easy interaction\n",
    "class PLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, proj_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder         # Take one-modality encoder from VLM.\n",
    "        self.proj_layer = proj_layer   # Take projection layer into joint embedding space.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Forward input trough encoder\n",
    "        features = self.encoder(**inputs).pooler_output # Forward trough encoder - we keep a global feature for the image/text\n",
    "        \n",
    "        # Project features\n",
    "        projected_features = self.proj_layer(features)  # Apply projection\n",
    "        \n",
    "        # Ensure image features are l2-norm\n",
    "        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True) # l2-normalization\n",
    "\n",
    "        return projected_features\n",
    "\n",
    "# Create model wrapper for vision and text encoders\n",
    "vision_encoder = PLIPWrapper(plip.vision_model, plip.visual_projection)\n",
    "text_encoder = PLIPWrapper(plip.text_model, plip.text_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe88a07-40c8-47cf-9df4-8f7ee4895f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:42:34.659497Z",
     "iopub.status.busy": "2024-07-08T15:42:34.659094Z",
     "iopub.status.idle": "2024-07-08T15:42:34.662920Z",
     "shell.execute_reply": "2024-07-08T15:42:34.662034Z",
     "shell.execute_reply.started": "2024-07-08T15:42:34.659467Z"
    },
    "tags": []
   },
   "source": [
    "#### **Test features extraction**\n",
    "First, we need to extract all feature representations from the test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b0947-e194-4b6c-8140-6a1c6d0aefc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To run over the whole dataset, we move now the vision model to gpu\n",
    "vision_encoder.to(device)\n",
    "\n",
    "# We need to format the pre-processing transforms in a more friendly format for torch Dataloaders\n",
    "from torchvision import transforms\n",
    "\n",
    "# Pre-processing transforms to apply during data loading.\n",
    "plip_transforms = transforms.Compose(\n",
    "    [\n",
    "    transforms.ToTensor(),                                                 # Move PIL/array image to tensor\n",
    "    transforms.Normalize(std=processor.image_processor.image_std,\n",
    "                         mean=processor.image_processor.image_mean),       # Intensity normalization\n",
    "    transforms.Resize(list(processor.image_processor.crop_size.values()))  # Resize to pre-trained resolution\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa197d2e-0f20-4776-9702-199390cc525b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataloader with the whole testing subset\n",
    "from vlms.data import loader\n",
    "test_loader = loader(dataframe_path=dataframe_test, path_images=path_images, categories=categories,\n",
    "                     transforms=plip_transforms, batch_size=8, num_workers=0)\n",
    "\n",
    "# We can check the dataset format and available samples\n",
    "print(\"Samples available for testing: \" + str(len(test_loader.dataset.data)))\n",
    "print(test_loader.dataset.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8099216-072e-4a3f-812d-79212bfaa717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "from vlms.utils import extract_features\n",
    "X_test, Y_test = extract_features(test_loader, vision_encoder)\n",
    "\n",
    "# Lets check the training dataset\n",
    "print(\"Test features: \" + str(X_test.shape))\n",
    "print(\"Test labels: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe14cf-2e21-45c4-b614-f4d839c0d428",
   "metadata": {},
   "source": [
    "#### **Compute text prototypes**\n",
    "(We will need them latter for classification head initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0de7e1-77e3-469b-8970-a7af3e13a2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensemble of templates\n",
    "templates = [\"a histopathology slide showing [CLS]\", \"histopathology image of [CLS]\",\n",
    "             \"pathology tissue showing [CLS]\", \"presence of [CLS] tissue on image\"]\n",
    "\n",
    "# Category-wise descriptions, which are more informative than category names. For instance, \"atrophic dense glands\" better \n",
    "# describes the local findings associated with Gleason grade 3.\n",
    "prompts_dict = {\"NC\": [\"benign glands\"],\n",
    "                \"G3\": [\"atrophic dense glands\"],\n",
    "                \"G4\": [\"cribriform ill-formed fused papillary patterns\"],\n",
    "                \"G5\": [\"isolated nest cells without lumen roseting patterns\"]}\n",
    "\n",
    "# Combine all paired options of templates and descriptions\n",
    "prompts = {}\n",
    "for iCategory in categories:\n",
    "    prompts[iCategory] = [caption.replace(\"[CLS]\", iDescription) for iDescription in prompts_dict[iCategory]\n",
    "                          for caption in templates]\n",
    "\n",
    "# Compute embeddings per category\n",
    "class_prototypes = []\n",
    "for iKey in range(len(categories)):\n",
    "    with torch.no_grad():\n",
    "        # Retrieve descriptions for that particular category\n",
    "        descriptions = prompts[categories[iKey]]\n",
    "        # Tokenize text\n",
    "        inputs = processor.tokenizer(descriptions, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "        # Forward text encoder\n",
    "        text_features_ensemble = text_encoder(inputs)\n",
    "        # Get class prototypes as average of all text prompts\n",
    "        avg_text_features = text_features_ensemble.mean(0).unsqueeze(0)\n",
    "        # Re-normalize embedding\n",
    "        avg_text_features = avg_text_features / avg_text_features.norm(dim=-1, keepdim=True)\n",
    "        class_prototypes.append(avg_text_features)\n",
    "                               \n",
    "# Concatenate all class prototypes\n",
    "zero_shot_prot = torch.concat(class_prototypes, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b5c34-83c6-4f01-a2a3-66a532b572bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T00:40:43.950882Z",
     "iopub.status.busy": "2024-07-10T00:40:43.950173Z",
     "iopub.status.idle": "2024-07-10T00:40:43.954379Z",
     "shell.execute_reply": "2024-07-10T00:40:43.953590Z",
     "shell.execute_reply.started": "2024-07-10T00:40:43.950843Z"
    },
    "tags": []
   },
   "source": [
    "#### **Few-shot training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b523b9f-28ff-430b-bc83-6f51c359ad95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vlms.data import few_shot_loader  # Take a look to this new function. We will randomly retrieve few samples for each class.\n",
    "shots, seed = 16, 1 # Define the number of shots per class for training and set reproducibility seed\n",
    "# Set data loader\n",
    "train_loader = few_shot_loader(dataframe_path=dataframe_train, path_images=path_images, categories=categories, transforms=plip_transforms,\n",
    "                               shots=shots, batch_size=32, num_workers=0, seed=seed)\n",
    "# Extract features\n",
    "X_train, Y_train = extract_features(train_loader, vision_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd6a7a-8b23-4b3e-a9c7-46313788d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T15:40:45.463612Z",
     "iopub.status.busy": "2024-07-08T15:40:45.463236Z",
     "iopub.status.idle": "2024-07-08T15:40:45.466830Z",
     "shell.execute_reply": "2024-07-08T15:40:45.466202Z",
     "shell.execute_reply.started": "2024-07-08T15:40:45.463582Z"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "#### **1.4.1. Linear Probing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c5fb3-03e3-4600-821e-77211f0d3eda",
   "metadata": {},
   "source": [
    "**The most straightforward adaptation strategy is training a logistic regression classifier, to learn new class prototypes using the few available shots**. This method is commonly called **Linear Probe** in the literature, and is employed to compare the transferability of pre-trained models. This method was the explored strategy on the seminal CLIP publication [1]. In the following, we implement and train such a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30198cef-0643-4659-8ca2-ec24c8dd6f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our Adapter class will be a module composed by: initialization, forward definition, and loss computing.\n",
    "# Init: we store the logit scale, and initialize a learnable set ot class prototypes.\n",
    "# Forward: compute softmax cosine similarity between current weights and input features.\n",
    "# loss: We minimize the categorical cross entropy as objective function during training.\n",
    "\n",
    "class LinearProbe(torch.nn.Module):\n",
    "    def __init__(self, input_features, number_classes, logit_scale):\n",
    "        super().__init__()\n",
    "        self.logit_scale = logit_scale\n",
    "        self.logit_scale.requires_grad = False\n",
    "        self.prototypes = torch.nn.Parameter(\n",
    "        torch.nn.init.kaiming_normal_(torch.empty((number_classes, input_features))))\n",
    "        # move to device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        # Get trained prototype\n",
    "        prototypes = self.prototypes.to(device)\n",
    "\n",
    "        # l2-normalized trained weights\n",
    "        prototypes_norm = prototypes / prototypes.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # temparature-scaled similarity per class\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = features @ prototypes_norm.t() * logit_scale\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd11a1-b70e-4b8a-bad6-69a47e77e801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the instance of linear probe with the number of features used and number of classes for prototypes.\n",
    "lp_adapter = LinearProbe(input_features=X_train.shape[-1], number_classes=len(categories),\n",
    "logit_scale=plip.logit_scale.detach().clone()) # Also, we need the same temperature scaling as in pre-training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f3e3d-6b3e-4fb2-b913-19984d7d7f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We train our adapter using few-shot data\n",
    "from vlms.utils import train_adapter # Take a look to this function, to train in mini-batches the Adapter.\n",
    "epochs, batch_size, learning_rate = 100, 32, 0.001 # Define training hyper-parameters\n",
    "optimizer = torch.optim.SGD(lp_adapter.parameters(), lr=learning_rate, momentum=0.9) # Define optimizer\n",
    "train_adapter(X_train, Y_train, lp_adapter, optimizer, batch_size, epochs) # Train adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2a84b-8cd6-4c5f-9f3b-341054eeb4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, we can test de resultant Adapter on test data. Since Adapters are light, we can do\n",
    "# a full-batch forward pass on test data.\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(lp_adapter(torch.tensor(X_test).to(device)), axis=-1).cpu().numpy()\n",
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538c3ed-414f-4f1a-990a-e5c314dbfcb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **ACTIVITY**\n",
    "First, try training the Adapter with 16 shots per class, and then decrease the number to 1 shot: what do you observe? **Which is the improvement of 1-shot adaptation with respect to zero-shot?** - **Try using different seeds**. In some datasets, the performance in the low-shot regime (k<4) was below zero-shot. Recently, Adapters that consider also text information, beyond randomly-initialized Linear Probe, have been considered to solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a9853-8b4b-4763-9e15-7cc93bdac314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T00:41:14.462884Z",
     "iopub.status.busy": "2024-07-10T00:41:14.462509Z",
     "iopub.status.idle": "2024-07-10T00:41:14.466066Z",
     "shell.execute_reply": "2024-07-10T00:41:14.465315Z",
     "shell.execute_reply.started": "2024-07-10T00:41:14.462858Z"
    },
    "tags": []
   },
   "source": [
    "------\n",
    "#### **1.4.2. CLIP-Adapter**\n",
    "\n",
    "The basic Linear Probe showed improvements detriment with respect to zero-shot in the initial studies. Since Linear Probe does not profit the text knowledge, some works explored more advanced options for black-box adaptation.\n",
    "\n",
    "Concretely, CLIP-Adapter [2] proposed to keep the text embeddings as text prototypes, and residually modify the vision features to approxiate the representations for their corresponding category. This residual modification is driven by a low-rank mlp arquitecture, and a blending hyper-parameter that controls how far you deviate from the initial representations. Concretely, CLIP-Adapter mlp module consist of:\n",
    "\n",
    "$$v' = (1-alpha) \\cdot v+alpha \\cdot mlp(v)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17d3f4-5fd6-4826-a454-ed37a6366345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPAdapter(torch.nn.Module):\n",
    "    def __init__(self, zero_shot_prot, logit_scale, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.logit_scale = logit_scale\n",
    "        self.logit_scale.requires_grad = False\n",
    "        self.zero_shot_prot = zero_shot_prot.clone().to(device) # Since it is not a parameter, we need to move it to the device ourselves\n",
    "        # The mlp residual Adapter that modifies the vision features:\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(zero_shot_prot.shape[-1], 4, bias=False),\n",
    "                                       torch.nn.ReLU(inplace=True),\n",
    "                                       torch.nn.Linear(4, zero_shot_prot.shape[-1], bias=False),\n",
    "                                       torch.nn.ReLU(inplace=True),)\n",
    "        # Alpha value for blending zero-shot and learned information on few shots.\n",
    "        self.alpha = alpha\n",
    "        # move to device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \n",
    "       # Residual adapter features: weighted residual modification of original features\n",
    "        features = (1-self.alpha) * features + self.alpha * self.mlp(features)\n",
    "\n",
    "        # Normalize output of feature adaptation and class prototype into an l2-norm space\n",
    "        image_features_norm = features / features.norm(dim=-1, keepdim=True)\n",
    "        prototypes_norm = self.zero_shot_prot / self.zero_shot_prot.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Logits: note that we keep the text prototypes as they were\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = image_features_norm @ prototypes_norm.t() * logit_scale\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecab7c-5dc6-4e78-a30d-3b6e06304fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the instance of CLIP-Adapter with a concrete alpha value\n",
    "alpha = 0.5\n",
    "CLIPAd_adapter = CLIPAdapter(zero_shot_prot=zero_shot_prot, logit_scale=plip.logit_scale.detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27b3d0-7552-4293-b810-530c3293dfb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We train our adapter using few-shot data\n",
    "from vlms.utils import train_adapter # Take a look to this function, to train in mini-batches the Adapter.\n",
    "epochs, batch_size, learning_rate = 100, 32, 0.001 # Define training hyper-parameters\n",
    "optimizer = torch.optim.SGD(CLIPAd_adapter.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_adapter(X_train, Y_train, CLIPAd_adapter, optimizer, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119630e-fbd3-4963-8653-345951415364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, we can test de resultant Adapter on test data. Since Adapter are light, we can do\n",
    "# a full-batch forward pass on test data.\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(CLIPAd_adapter(torch.tensor(X_test).to(device)), axis=-1).cpu().numpy()\n",
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5982a90-41bf-45f1-ac6e-6012d0b866ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T18:36:59.741396Z",
     "iopub.status.busy": "2024-07-10T18:36:59.740990Z",
     "iopub.status.idle": "2024-07-10T18:36:59.746171Z",
     "shell.execute_reply": "2024-07-10T18:36:59.745516Z",
     "shell.execute_reply.started": "2024-07-10T18:36:59.741357Z"
    },
    "tags": []
   },
   "source": [
    "##### **NOTE**\n",
    "As you can see, **CLIP-Adapter prevents the performance drop when K=1**. Now, try using **different number of shots, and values for the alpha hyper-parameter**. What limitations do you observe? **How can you properly fix alpha value in a few-shot setting**, without using test data feedback? As demonstrated in [3], the value of this configuration is dataset-dependant, which is unrealistic in the few-shot setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beda4ad-2227-43f9-8aa3-f6c4923ac97e",
   "metadata": {},
   "source": [
    "------\n",
    "#### **1.4.3. Zero-shot initialized Linear Probe**\n",
    "\n",
    "Motivated by the **absence of model selection strategies in CLIP-Adapter and other methods**, the work in [3] revisits few-shot adaptation of vision-language models.\n",
    "\n",
    "Concretely, one observation is that **the limited performance of Linear Probing was explained by the random initialization of the new class prototypes**. Indeed, employing the **text-driven class prototypes as initial trained weights is competitive with more convoluted methods.\n",
    "\n",
    "Lets train ZS-LP [3], this well-initialized Linear Probe. Remember checking the 1-shot case, and compare the performance with respect to the previously introduced Linear Probing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b740019-89d1-40c5-99cd-b5fb682f264c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remember the shape of the text-driven prototypes: classes x features.\n",
    "print(zero_shot_prot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f861c9c-0837-46c4-aeea-634bd6a4210d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Linear Probing class takes zero-shot prototypes as input to initialize the prototypes\n",
    "class ZSLinearProbe(torch.nn.Module):\n",
    "    def __init__(self, zero_shot_prot, logit_scale):\n",
    "        super().__init__()\n",
    "        # We keep the same temperature scaling, but we do not train it any more\n",
    "        self.logit_scale = logit_scale\n",
    "        self.logit_scale.requires_grad = False\n",
    "        # Initialize prototypes with zero-shot weights\n",
    "        self.prototypes = torch.nn.Parameter(zero_shot_prot.clone())\n",
    "        # move to device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        # Get trained prototype\n",
    "        prototypes = self.prototypes.to(device)\n",
    "\n",
    "        # l2-normalized trained weights\n",
    "        prototypes_norm = prototypes / prototypes.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # temparature-scaled similarity per class\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = features @ prototypes_norm.t() * logit_scale\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cf909-5772-4fd4-a2cf-33e2bacfe72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the instance of linear probe initialized with zero-shot weights\n",
    "zslp_adapter = ZSLinearProbe(zero_shot_prot=zero_shot_prot, logit_scale=plip.logit_scale.detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a064f2-2ad4-4fa7-b3d7-5397731215bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We train our adapter using few-shot data\n",
    "from vlms.utils import train_adapter # Take a look to this function, to train in mini-batches the Adapter.\n",
    "epochs, batch_size, learning_rate = 100, 32, 0.001 # Define training hyper-parameters\n",
    "optimizer = torch.optim.SGD(zslp_adapter.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_adapter(X_train, Y_train, zslp_adapter, optimizer, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779221ff-e25c-49c5-897f-a48f4e07ac71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, we can test de resultant Adapter on test data. Since Adapter are light, we can do\n",
    "# a full-batch forward pass on test data.\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(zslp_adapter(torch.tensor(X_test).to(device)), axis=-1).cpu().numpy()\n",
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69264-6bdd-4f10-9d78-d438c5810599",
   "metadata": {},
   "source": [
    "------\n",
    "#### **1.4.4. Class-Adaptive Linear Probing (CLAP)**\n",
    "\n",
    "Finally, we will check the **state-of-the-art method for black-box adaptation of vision-language models, CLAP [3]**. The motivation of CLAP is adaptively **retain the robust zero-shot prototypes when updating the new ones using few-shots**. The idea is quite straightforward: **if the zero-shot prototypes performs well, why would you want to go far from it?** This is solved by constraining the learned prototypes to stay close to the initial solution. The overall loss function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\phantom{.}\\min_{\\mathcal{W}}  \\quad\n",
    "\\sum\\limits_{i \\in \\mathcal{S}} \\mathcal{H}_{ce}({\\mathbf{y}^{(i)},\\hat{\\mathbf{y}}^{(i)}}) +\n",
    "\\sum_{c=1}^C \\lambda_{c} \\; ||\\mathbf{t}_c - \\mathbf{w}_{c}||_{2}^{2}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{H}_{ce}$ is cross-entropy loss on predictions with the learned prototypes, and the second term is an **l2-penalty**, which provides **large values if the learned prototypes, $\\mathbf{w}$, deviate from the text prototypes, $\\mathbf{t}_c$**. Note that we are minimizing this loss function, so we will also minimize the penalty/deviation. Importantly, this is done class-wise (for each $c$ of the $C$ categories).\n",
    "\n",
    "The zero-shot prototypes might be stronger for one categories than for others. To consider this, **CLAP uses class-wise weights that control the relevance of the penalty**. This weight is directly **estimated by quantifying the quality of these prototypes on support data** (few-shots) before training.\n",
    "\n",
    "You can know more about this and balck-box Adapters here: [https://github.com/jusiro/CLAP](https://github.com/jusiro/CLAP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b22e8-a6f3-4a0a-b19c-d02bae6ed993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLAP(torch.nn.Module):\n",
    "    def __init__(self, zero_shot_prot, logit_scale):\n",
    "        super().__init__()\n",
    "        # We use the same temperature scaling value as in pre-training\n",
    "        self.logit_scale = logit_scale\n",
    "        self.logit_scale.requires_grad = False\n",
    "        # Trained weights W, which we initialize with zero-shot weights.\n",
    "        self.prototypes = torch.nn.Parameter(zero_shot_prot.clone())\n",
    "        # Zero-shot prototypes, t, which we will use as anchor for the penalty term.\n",
    "        self.anchors = torch.nn.Parameter(zero_shot_prot.clone())\n",
    "        self.anchors.requires_grad = False\n",
    "        # Init penaly weights (we will initialize them latter, once we get train data)\n",
    "        self.lambdas = torch.zeros((zero_shot_prot.shape[0])).to(device)\n",
    "        # move to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def init_multipliers(self, X_train, Y_train):\n",
    "        # Function to compute the initial multipliers value:\n",
    "        # 1. Get predictions (softmax outputs) from train data\n",
    "        # 2. Take average softmax value for each category\n",
    "        # (Idea): larger avg. softmax for the correct category, better the model is and larger the\n",
    "        # penalty if you deviate (lambda -> 1). The lower the softmax output in average, the worsr\n",
    "        # the zero-shot prototype for this category is, and the lower the penalty if you deviate \n",
    "        # (lambda -> 0)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Move to device inputs\n",
    "            X_train = torch.tensor(X_train).to(device)\n",
    "            Y_train = torch.tensor(Y_train).to(device)\n",
    "\n",
    "            # Compute logits in train data\n",
    "            logits = self.forward(X_train)\n",
    "\n",
    "            # Pass Y_train to one-hot to compute average [3] -> [0, 0, 0, 1]\n",
    "            labels_one_hot = torch.nn.functional.one_hot(Y_train)\n",
    "\n",
    "            # Estimate the quality of the zero-shot protoypes per class / average per class\n",
    "            anchors_q = torch.diag(torch.softmax(logits, -1).t() @ labels_one_hot.to(torch.float32)) / \\\n",
    "                    labels_one_hot.sum(0)\n",
    "\n",
    "            # Init new lambdas\n",
    "            self.lambdas = torch.clone(anchors_q).to(device)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Note that the forward pass is the same as in Linear Probing!!\n",
    "        \n",
    "        # Get trained prototype\n",
    "        prototypes = self.prototypes.to(device)\n",
    "\n",
    "        # l2-normalized trained weights\n",
    "        prototypes_norm = prototypes / prototypes.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # temparature-scaled similarity per class\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = features @ prototypes_norm.t() * logit_scale\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        # Cross-entropy on labels and predictions\n",
    "        ce_loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        # L2-penalty (distance between vectors) for base class prototypes\n",
    "        penalty = (self.prototypes - self.anchors).pow(2).sum(-1)\n",
    "        # Weight with class-wise multipliers\n",
    "        weighted_penalty = torch.mean(self.lambdas * penalty)\n",
    "        # Compute overall loss as the sum of both terms\n",
    "        loss = ce_loss + weighted_penalty\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89f413-445e-483d-a88d-b67377fa617b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the instance of linear probe initialized with zero-shot weights\n",
    "clap_adapter = CLAP(zero_shot_prot=zero_shot_prot, logit_scale=plip.logit_scale.detach().clone())\n",
    "# Init multipliers\n",
    "clap_adapter.init_multipliers(X_train, Y_train)\n",
    "print(\"Lambda multipliers: \" + str(clap_adapter.lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4ff71-dbd1-4706-b45f-6837c65abc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We train our adapter using few-shot data\n",
    "from vlms.utils import train_adapter # Take a look to this function, to train in mini-batches the Adapter.\n",
    "epochs, batch_size, learning_rate = 100, 32, 0.001 # Define training hyper-parameters\n",
    "optimizer = torch.optim.SGD(clap_adapter.parameters(), lr=learning_rate, momentum=0.9)\n",
    "train_adapter(X_train, Y_train, clap_adapter, optimizer, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64dc749-8af7-46d5-a41d-d4e4c09d6fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, we can test de resultant Adapter on test data. Since Adapters are light, we can do\n",
    "# a full-batch forward pass on test data.\n",
    "with torch.no_grad():\n",
    "    prob = torch.softmax(clap_adapter(torch.tensor(X_test).to(device)), axis=-1).cpu().numpy()\n",
    "# Compute metrics\n",
    "from vlms.utils import evaluate\n",
    "aca, cm = evaluate(Y_test, prob)\n",
    "print(\"Balanced accuracy: \" + str(aca))\n",
    "print(\"Confusion matrix: \")\n",
    "print(str(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0bcfb-300a-4b96-b0a0-84173d3e8725",
   "metadata": {},
   "source": [
    "--- \n",
    "##### **ACTIVITY**\n",
    "\n",
    "Well, now you know everything you need about black-box Adapters. If you want to know more, I reccomend:\n",
    "\n",
    "- Try different random seeds, since few-shot transferability might present large variability in performance depending on the chosen samples.\n",
    "- Try developing the same pipeline for [CONCH](https://huggingface.co/MahmoodLab/CONCH) [4], a revently introduced VLM for histology. Its vision backbone is large scale, takes large-resolution input images, and is pre-trained with more data. How does model scaling translates to black-box Adaptation?\n",
    "\n",
    "\n",
    "--- \n",
    "## **References**\n",
    "\n",
    "\n",
    "[1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning. \\\n",
    "[2] Gao, P., Geng, S., Zhang, R. et al. (2024). CLIP-Adapter: Better Vision-Language Models with Feature Adapters. Int J Comput Vis. \\\n",
    "[3] Silva-Rodriguez, J., Hajimiri, S., Ben Ayed, I., Dolz, J. (2024). A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). \\\n",
    "[4] Lu, M.Y., Chen, B., Williamson, D.F.K. et al. (2024) A visual-language foundation model for computational pathology. Nature Medicine.\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
