{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f43f63-2972-4103-9861-c2a0ef9b0d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T00:44:40.883251Z",
     "iopub.status.busy": "2024-07-11T00:44:40.882870Z",
     "iopub.status.idle": "2024-07-11T00:44:40.888000Z",
     "shell.execute_reply": "2024-07-11T00:44:40.887154Z",
     "shell.execute_reply.started": "2024-07-11T00:44:40.883221Z"
    },
    "tags": []
   },
   "source": [
    "### **Vision-Language Medical Foundation Models**\n",
    "\n",
    "#### **1.2. Contrastive text-image pre-training**\n",
    "---\n",
    "\n",
    "**The most popular paradigm for vision-language pre-training was introduced in CLIP [1]**. Given a dataset with **paired images and text descriptions**, a **vision and a text encoder** are pre-trained to **produce a joint embedding space** in which paired data propuce similar representation, which are pushed away from unpaired samples.\n",
    "\n",
    "Note that pre-training is time-consuming, and you require a large dataset with image-text pairs, which are scarce in medical imaging. In this notebook, we will simply do a toy example to compute the CLIP **contrastive language-image pre-training loss**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee95385-22a2-49ef-9f62-d6e26e3997d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Device for training/inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Available device: \" + device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10155cd2-7a75-432f-80dc-241f7cd5dd26",
   "metadata": {},
   "source": [
    "#### **VLM model wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d67c1-ad67-4e8b-96d0-d1e42b245295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model and pre-processing tools from huggingface\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# In Transformers library, models and versions are storages by an ID defininf the use and model name.\n",
    "# For PLIP model, such ID is \"vinid/plip\"\n",
    "processor = AutoProcessor.from_pretrained(\"vinid/plip\") # pre-processing image and text\n",
    "processor.image_processor.do_center_crop = False\n",
    "plip = AutoModel.from_pretrained(\"vinid/plip\").eval() # model with pre-trained weights\n",
    "# We set model in eval mode to avoid droput inference and batchnorm stats update in CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca83727-492a-4cb0-886b-99fac9e9f2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Again, we will use our PLIP Wrapper for easy interaction\n",
    "class PLIPWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder, proj_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder         # Take one-modality encoder from VLM.\n",
    "        self.proj_layer = proj_layer   # Take projection layer into joint embedding space.\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Forward input trough encoder\n",
    "        features = self.encoder(**inputs).pooler_output # Forward trough encoder - we keep a global feature for the image/text\n",
    "        \n",
    "        # Project features\n",
    "        projected_features = self.proj_layer(features)  # Apply projection\n",
    "        \n",
    "        # Ensure image features are l2-norm\n",
    "        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True) # l2-normalization\n",
    "\n",
    "        return projected_features\n",
    "\n",
    "# Create model wrapper for vision and text encoders\n",
    "vision_encoder = PLIPWrapper(plip.vision_model, plip.visual_projection)\n",
    "text_encoder = PLIPWrapper(plip.text_model, plip.text_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bbdf5-7a9b-40a9-a6e0-ba20cbc6ffbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T00:45:19.478333Z",
     "iopub.status.busy": "2024-07-11T00:45:19.477931Z",
     "iopub.status.idle": "2024-07-11T00:45:19.481960Z",
     "shell.execute_reply": "2024-07-11T00:45:19.481110Z",
     "shell.execute_reply.started": "2024-07-11T00:45:19.478306Z"
    },
    "tags": []
   },
   "source": [
    "#### **Contrastive pre-training**\n",
    "\n",
    "Now, we are going to compute the CLIP pre-training loss. We will do an example with few images, and naive text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f3fa0-3085-4f55-92f3-d33f1072d347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fist, lets load  few examples for each category.\n",
    "nc = Image.open(\"./local_data/datasets/SICAPv2/images/16B0028148_Block_Region_8_2_14_xini_27858_yini_55555.jpg\") \n",
    "g3 = Image.open(\"./local_data/datasets/SICAPv2/images/18B0006169B_Block_Region_6_3_7_xini_33958_yini_90365.jpg\") \n",
    "g4 = Image.open(\"./local_data/datasets/SICAPv2/images/17B0032153_Block_Region_10_13_17_xini_23105_yini_15687.jpg\") \n",
    "g5 = Image.open(\"./local_data/datasets/SICAPv2/images/16B0008067_Block_Region_0_6_2_xini_10859_yini_103113.jpg\") \n",
    "\n",
    "# Now, we can read the images, pre-processing then, and concatenate as a batched input.\n",
    "image_inputs = processor.image_processor([nc, g3, g4, g5], return_tensors=\"pt\")\n",
    "print(image_inputs['pixel_values'].shape)\n",
    "\n",
    "# Now, we can produce some naive text descriptions for the toy example, which would be \"paired\" with each image\n",
    "prompt = [\"Healthy prostate tissue\",\n",
    "          \"Gleason grade 3\",\n",
    "          \"Gleason grade 4\",\n",
    "          \"Gleason grade 5\"]\n",
    "\n",
    "# Pre-process text data\n",
    "text_inputs = processor.tokenizer(prompt, max_length = 77, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "\n",
    "# Forward representations into common space\n",
    "with torch.no_grad():\n",
    "    image_features = vision_encoder(image_inputs)\n",
    "    text_features = text_encoder(text_inputs)\n",
    "\n",
    "# Compute similarity matrix (matrix multiplication v(bs x D) @ t(D x bs) -> (bs x bs))\n",
    "# This matrix represents, for each row, the similarity of the image ith to the text jth.\n",
    "# Ideally, we want large similarity in the diagonal (image i=0 with text j=0, this is, paired).\n",
    "# Also, we want smaller similarity in elements out of the diagonal.\n",
    "# The contrastive term does so, from images to texts (i.e. per rows) and text to images (i.e. per columns).\n",
    "with torch.no_grad():\n",
    "    sim = image_features @ text_features.t() * plip.logit_scale.exp()\n",
    "print(\"Predicted similarity matrix\")\n",
    "print(sim)\n",
    "print(\" \")\n",
    "\n",
    "# One-to-One Target\n",
    "target = torch.eye(text_features.shape[0]).detach()  # Create target similarity matrix\n",
    "print(\"Target similarity matrix\")\n",
    "print(target)\n",
    "print(\" \")\n",
    "\n",
    "# Image-to-text loss:\n",
    "# 1.Compute softmax over rows.\n",
    "# 2. Apply cross-entropy, being the target for each sample the intex of its paired text.\n",
    "logits_per_image = sim\n",
    "print(\"I2T Softmax:\")\n",
    "with torch.no_grad():\n",
    "    print(str(torch.softmax(logits_per_image, dim=-1).numpy().round(2)))\n",
    "    print(\" \")\n",
    "i2t_loss = torch.nn.functional.cross_entropy(logits_per_image, target)\n",
    "\n",
    "# Text-to-image loss:\n",
    "# 1.Compute softmax over columns.\n",
    "# 2. Apply cross-entropy, being the target for each text the inted of its paired image.\n",
    "logits_per_text = logits_per_image.t()\n",
    "print(\"T2I Softmax:\")\n",
    "with torch.no_grad():\n",
    "    print(str(torch.softmax(logits_per_text, dim=-1).numpy().round(2)))\n",
    "    print(\" \")\n",
    "t2i_loss = torch.nn.functional.cross_entropy(logits_per_text, target)\n",
    "\n",
    "# Overall clip loss\n",
    "clip_loss = (i2t_loss + t2i_loss) / 2\n",
    "print(\"CLIP Loss: \" + str(clip_loss.item()))\n",
    "\n",
    "# This loss is computed on mini-batches, and is backpropagated trough both encoders. \n",
    "# During training, the joint embedding space will be trained to minimize CLIP loss, which will align\n",
    "# paired concepts in both modalities. It is worth mentioning that the temperature scaling parameter\n",
    "# is also optimized during pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae6af5-8141-4954-bdfe-7e3bed9c6932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T23:48:38.013986Z",
     "iopub.status.busy": "2024-07-10T23:48:38.013591Z",
     "iopub.status.idle": "2024-07-10T23:48:38.019305Z",
     "shell.execute_reply": "2024-07-10T23:48:38.018365Z",
     "shell.execute_reply.started": "2024-07-10T23:48:38.013959Z"
    },
    "tags": []
   },
   "source": [
    "## **References**\n",
    "\n",
    "\n",
    "[1] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. International Conference on Machine Learning. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
